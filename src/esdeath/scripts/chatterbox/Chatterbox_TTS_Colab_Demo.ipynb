{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -e\n",
        "\n",
        "cd /content\n",
        "\n",
        "# Download micromamba into /content/bin/micromamba\n",
        "curl -Ls https://micro.mamba.pm/api/micromamba/linux-64/latest | tar -xvj bin/micromamba\n",
        "\n",
        "# Create a clean env (isolated from Colab’s global packages)\n",
        "./bin/micromamba create -y -n cb311 -c conda-forge python=3.11 pip\n",
        "\n",
        "echo \"✅ micromamba ready at /content/bin/micromamba\"\n",
        "echo \"✅ env created: cb311\"\n"
      ],
      "metadata": {
        "id": "tsDDevyVuCOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -euo pipefail\n",
        "\n",
        "cd /content\n",
        "MICROMAMBA=\"/content/bin/micromamba\"\n",
        "\n",
        "ts() { date +\"[%Y-%m-%d %H:%M:%S]\"; }\n",
        "\n",
        "echo \"$(ts) Sanity check micromamba:\"\n",
        "ls -lah \"$MICROMAMBA\"\n",
        "\n",
        "echo \"$(ts) Upgrading pip tooling inside cb311...\"\n",
        "\"$MICROMAMBA\" run -n cb311 python -m pip install -U pip setuptools wheel --progress-bar on\n",
        "\n",
        "echo \"$(ts) Installing PyTorch 2.5.1 (CUDA 12.1)... (this can take a while)\"\n",
        "\"$MICROMAMBA\" run -n cb311 pip install \\\n",
        "  --progress-bar on \\\n",
        "  torch==2.5.1+cu121 torchaudio==2.5.1+cu121 torchvision==0.20.1+cu121 \\\n",
        "  --index-url https://download.pytorch.org/whl/cu121\n",
        "\n",
        "echo \"$(ts) Installing ONNX (wheel)...\"\n",
        "\"$MICROMAMBA\" run -n cb311 pip install --progress-bar on onnx==1.16.0\n",
        "\n",
        "echo \"$(ts) Installing Chatterbox package (from GitHub, no-cache, upgrade)...\"\n",
        "\"$MICROMAMBA\" run -n cb311 pip uninstall -y chatterbox-tts chatterbox || true\n",
        "\"$MICROMAMBA\" run -n cb311 pip install \\\n",
        "  --no-cache-dir --upgrade \\\n",
        "  --progress-bar on \\\n",
        "  \"chatterbox-tts @ git+https://github.com/devnen/chatterbox-v2.git@master\"\n",
        "\n",
        "echo \"$(ts) ✅ Installation complete!\"\n"
      ],
      "metadata": {
        "id": "2alxTXqZuGhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "set -e\n",
        "\n",
        "/content/bin/micromamba run -n cb311 python - <<'PY'\n",
        "import inspect, torch\n",
        "import chatterbox.tts_turbo as t\n",
        "\n",
        "print(\"✅ torch:\", torch.__version__)\n",
        "print(\"✅ cuda available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"✅ gpu:\", torch.cuda.get_device_name(0))\n",
        "\n",
        "print(\"✅ chatterbox.tts_turbo path:\", t.__file__)\n",
        "\n",
        "src = inspect.getsource(t.ChatterboxTurboTTS.from_pretrained)\n",
        "print(\"\\n--- from_pretrained() (first ~80 lines) ---\")\n",
        "print(\"\\n\".join(src.splitlines()[:80]))\n",
        "\n",
        "# Heuristic check for the common buggy pattern that forces token=True semantics\n",
        "markers = [\" or True\", \"token=True\", \"token = True\", \"use_auth_token=True\"]\n",
        "hits = [m for m in markers if m in src]\n",
        "print(\"\\nHeuristic auth-forcing markers found:\", hits)\n",
        "\n",
        "if hits:\n",
        "    raise SystemExit(\n",
        "        \"\\n❌ This install still appears to force HF auth.\\n\"\n",
        "        \"Re-run Cell 2 (it already uses --no-cache-dir --upgrade).\\n\"\n",
        "    )\n",
        "\n",
        "print(\"\\n✅ Looks good: Turbo should download without requiring user tokens.\")\n",
        "PY"
      ],
      "metadata": {
        "id": "d1slxgHuuLNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title 4. Install Server + Run With Full Live Logs (foreground)\n",
        "import os, time, subprocess, socket, requests\n",
        "from pathlib import Path\n",
        "\n",
        "PORT = 8004\n",
        "REPO_DIR = \"/content/Chatterbox-TTS-Server\"\n",
        "LOG_STDOUT = \"/content/chatterbox_server_stdout.log\"\n",
        "\n",
        "def sh(cmd, check=False):\n",
        "    return subprocess.run([\"bash\", \"-lc\", cmd], check=check)\n",
        "\n",
        "def port_open(host=\"127.0.0.1\", port=PORT, timeout=0.25):\n",
        "    try:\n",
        "        with socket.create_connection((host, port), timeout=timeout):\n",
        "            return True\n",
        "    except OSError:\n",
        "        return False\n",
        "\n",
        "os.chdir(\"/content\")\n",
        "\n",
        "# Fresh clone\n",
        "sh(\"rm -rf /content/Chatterbox-TTS-Server\", check=False)\n",
        "sh(\"git clone https://github.com/devnen/Chatterbox-TTS-Server.git\", check=True)\n",
        "os.chdir(REPO_DIR)\n",
        "\n",
        "print(\"=== Quick system checks ===\")\n",
        "sh(\"nvidia-smi || true\", check=False)\n",
        "\n",
        "print(\"\\n=== Installing server requirements (prefer repo pins if present) ===\")\n",
        "if Path(\"requirements-nvidia.txt\").exists():\n",
        "    sh(\"/content/bin/micromamba run -n cb311 pip install -U pip setuptools wheel\", check=False)\n",
        "    sh(\"/content/bin/micromamba run -n cb311 pip install -r requirements-nvidia.txt\", check=False)\n",
        "else:\n",
        "    sh(\n",
        "        \"/content/bin/micromamba run -n cb311 pip install -U pip setuptools wheel && \"\n",
        "        \"/content/bin/micromamba run -n cb311 pip install \"\n",
        "        \"fastapi 'uvicorn[standard]' pyyaml soundfile librosa safetensors \"\n",
        "        \"python-multipart requests jinja2 watchdog aiofiles unidecode inflect tqdm \"\n",
        "        \"pydub audiotsm praat-parselmouth\",\n",
        "        check=False\n",
        "    )\n",
        "\n",
        "print(\"\\n=== Removing old stdout log ===\")\n",
        "Path(LOG_STDOUT).unlink(missing_ok=True)\n",
        "\n",
        "print(\"\\n=== Starting server with LIVE logs ===\")\n",
        "print(\"Log file:\", LOG_STDOUT)\n",
        "print(\"To stop the server, run Cell 5.\\n\")\n",
        "\n",
        "env = os.environ.copy()\n",
        "env[\"PYTHONUNBUFFERED\"] = \"1\"\n",
        "\n",
        "# Put HF cache somewhere inspectable/persistent for this runtime\n",
        "env[\"HF_HOME\"] = \"/content/hf_home\"\n",
        "env[\"TRANSFORMERS_CACHE\"] = \"/content/hf_home/transformers\"\n",
        "env[\"HF_HUB_CACHE\"] = \"/content/hf_home/hub\"\n",
        "Path(env[\"HF_HOME\"]).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "proc = subprocess.Popen(\n",
        "    [\"/content/bin/micromamba\", \"run\", \"-n\", \"cb311\", \"python\", \"-u\", \"server.py\"],\n",
        "    stdout=subprocess.PIPE,\n",
        "    stderr=subprocess.STDOUT,\n",
        "    text=True,\n",
        "    bufsize=1,\n",
        "    env=env,\n",
        ")\n",
        "\n",
        "with open(LOG_STDOUT, \"w\", encoding=\"utf-8\", errors=\"replace\") as f:\n",
        "    shown_link = False\n",
        "    while True:\n",
        "        line = proc.stdout.readline()\n",
        "        if line:\n",
        "            print(line, end=\"\")\n",
        "            f.write(line)\n",
        "            f.flush()\n",
        "\n",
        "        if (not shown_link) and port_open():\n",
        "            shown_link = True\n",
        "            print(\"\\n=== Server port is reachable ===\")\n",
        "            print(\"Click the Colab proxy link below to open the Web UI.\")\n",
        "            from google.colab.output import serve_kernel_port_as_window\n",
        "            serve_kernel_port_as_window(PORT)\n",
        "\n",
        "            # Verify model load status via server endpoint\n",
        "            try:\n",
        "                mi = requests.get(f\"http://127.0.0.1:{PORT}/api/model-info\", timeout=2).json()\n",
        "                print(\"\\n/api/model-info:\", mi)\n",
        "            except Exception as e:\n",
        "                print(\"\\n/api/model-info query failed:\", repr(e))\n",
        "\n",
        "        if proc.poll() is not None:\n",
        "            print(\"\\n=== Server process exited with code\", proc.returncode, \"===\")\n",
        "            break"
      ],
      "metadata": {
        "id": "n8DXZwq1uQEB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "PORT=8004\n",
        "\n",
        "echo \"PIDs listening on port $PORT:\"\n",
        "sudo lsof -t -i:$PORT || true\n",
        "\n",
        "echo \"Killing...\"\n",
        "sudo lsof -t -i:$PORT | xargs -r sudo kill -9\n",
        "\n",
        "echo \"Verify nothing is listening:\"\n",
        "sudo lsof -i:$PORT || true\n"
      ],
      "metadata": {
        "id": "SvSnFn9AuTVd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
